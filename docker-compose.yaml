version: "3.8"

services:
  fetcher:
    build:
      context: ./src/extractor
      dockerfile: Dockerfile
    container_name: fetcher
    ports:
      - '7000:7000'
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - project_network
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    volumes:
      - ./src/extractor/connector.py:/app/src/connector.py
    restart: on-failure

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - project_network

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"      # Internal
      - "29092:29092"    # Host access
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    entrypoint: |
      sh -c "
        # Start Kafka in background
        /etc/confluent/docker/run &
        # Wait for Kafka to be up (you might need to adjust the sleep duration)
        echo 'Waiting for Kafka to start...'; sleep 15;
        # Create Kafka topic
        kafka-topics --create \
          --topic cryptocurrency \
          --bootstrap-server localhost:9092 \
          --partitions 1 \
          --replication-factor 1;
        # Wait for background process to end
        wait
      "
    networks:
      - project_network
    healthcheck:
      test: [ "CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list" ]
      interval: 10s
      timeout: 5s
      retries: 10

  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    user: "0:0"
    networks:
      - project_network
    depends_on:
      kafka:
        condition: service_healthy
      cassandra:
        condition: service_healthy
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_OPTS=-Dspark.deploy.defaultCores=1
      - SPARK_DAEMON_MEMORY=2G
    volumes:
      - ./src/consumerr/:/opt/bitnami/spark/venv/src/
    command: |
      bash -c "
        mkdir -p /opt/bitnami/spark/venv/src/checkpoint 
        pip install --no-cache-dir -r /opt/bitnami/spark/venv/src/requirements.txt
        /opt/bitnami/spark/sbin/start-master.sh

        # Wait for master to be up
        echo 'Waiting for Spark master to be ready...'
        sleep 30

        # Keep container running
        tail -f /opt/bitnami/spark/logs/spark-*-org.apache.spark.deploy.master.Master-*.out
      "

  spark-worker-1:
    image: bitnami/spark:3.5
    container_name: spark-worker-1
    networks:
      - project_network
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1g
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_OPTS=-Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.interval=3600
    volumes:
      - spark_worker1_logs:/opt/bitnami/spark/logs
      - spark_worker1_work:/opt/bitnami/spark/work
    depends_on:
      - spark-master
    healthcheck:
      test: ps aux | grep "[o]rg.apache.spark.deploy.worker.Worker" || exit 1
      interval: 10s
      timeout: 5s
      retries: 3

  spark-worker-2:
    image: bitnami/spark:3.5
    container_name: spark-worker-2
    networks:
      - project_network
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1g
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_OPTS=-Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.interval=3600
    volumes:
      - spark_worker2_logs:/opt/bitnami/spark/logs
      - spark_worker2_work:/opt/bitnami/spark/work
    depends_on:
      - spark-master
    healthcheck:
      test: ps aux | grep "[o]rg.apache.spark.deploy.worker.Worker" || exit 1
      interval: 10s
      timeout: 5s
      retries: 3

  cassandra:
    image: cassandra:latest
    container_name: cassandra
    hostname: cassandra
    ports:
      - "7006:7006"
      - "9042:9042"
    environment:
      - CASSANDRA_CLUSTER_NAME=CryptoCluster
      - CASSANDRA_DC=DC1
      - CASSANDRA_RACK=Rack1
      - CASSANDRA_NUM_TOKENS=256
      - CASSANDRA_LISTEN_ADDRESS=auto
      - CASSANDRA_BROADCAST_ADDRESS=cassandra
      - CASSANDRA_BROADCAST_RPC_ADDRESS=cassandra
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
    volumes:
      - cassandra_data:/var/lib/cassandra
    healthcheck:
      test: ["CMD", "nodetool", "status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - project_network

  grafana:
   image: grafana/grafana:latest
   container_name: grafana
   ports:
    - "3000:3000"
   environment:
    - GF_SECURITY_ADMIN_USER=admin
    - GF_SECURITY_ADMIN_PASSWORD=admin
    - GF_INSTALL_PLUGINS=hadesarchitect-cassandra-datasource
   volumes:
    - grafana_data:/var/lib/grafana
   networks:
    - project_network
   depends_on:
    - cassandra
    - spark-master

volumes:
  cassandra_data:
    driver: local
  spark_logs:
    driver: local
  spark_work:
    driver: local
  spark_worker1_logs:
    driver: local
  spark_worker1_work:
    driver: local
  spark_worker2_logs:
    driver: local
  spark_worker2_work:
    driver: local
  grafana_data:
    driver: local

networks:
  project_network:
    driver: bridge